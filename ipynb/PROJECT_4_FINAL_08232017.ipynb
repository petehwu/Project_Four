{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2 in /opt/conda/lib/python3.6/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/lib\n"
     ]
    }
   ],
   "source": [
    "cd '/home/jovyan/lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/lib'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from db_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary for Semantic Search Postgres Database\n",
    "\n",
    "#### Table Name:  PAGE_INFO\n",
    "    Column Name: PAGEID\n",
    "    Column Type: INTEGER PRIMARY KEY\n",
    "    Column Desc: PAGEID from wikipedia\n",
    "\n",
    "    Column Name: TITLE\n",
    "    Column Type: TEXT\n",
    "    Column Desc: Title of wikipedia webpage\n",
    "\n",
    "#### Table Name: CATEGORY_INFO\n",
    "    Column Name: CATEGORYID\n",
    "    Column Type: INTEGER PRIMARY KEY\n",
    "    Column Desc: Category Unique identifier.  Postgres auto increment Integer field\n",
    "    \n",
    "    Column Name: CATEGORY_NAME\n",
    "    Column Type: TEXT\n",
    "    Column Desc: Category Name\n",
    "        \n",
    "#### Table Name: PAGE_DATA \n",
    "    Column Name: PAGEID \n",
    "    Column Type: INTEGER PRIMARY KEY,\n",
    "    Column Desc: PAGEID from wikipedia\n",
    "    \n",
    "    Column Name: TITLE \n",
    "    Column Type: TEXT\n",
    "    Column Desc: wikipedia Page title\n",
    "    \n",
    "    Column Name: PAGE_TEXT \n",
    "    Column Type: TEXT\n",
    "    Column Desc: wikipedia page text(cleaned)\n",
    "        \n",
    "#### Table Name: CATEGORY_DATA \n",
    "    Column Name: CID\n",
    "    Column Type: INTEGER PRIMARY KEY\n",
    "    Column Desc: Unique Record identifier.  Postgres auto increment Integer field\n",
    "    \n",
    "    Column Name: MAINCATEGORYID \n",
    "    Column Type: INTEGER \n",
    "    Column Desc: CategoryID of the Main Category\n",
    "    \n",
    "    Column Name: PAGEID \n",
    "    Column Type: INTEGER\n",
    "    Column Desc: PAGEID from wikipedia\n",
    "    \n",
    "    Column Name: CATEGORY\n",
    "    Column Type: TEXT\n",
    "    Column Desc: Category Name\n",
    "    \n",
    "    Column Name: SUBCATEGORY \n",
    "    Column Type: TEXT\n",
    "    Column Desc: Sub-Category Name\n",
    "    \n",
    "    Column Name: LEVEL \n",
    "    Column Type: INTEGER\n",
    "    Column Desc: sub-level from Main Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clear_table = \"\"\"\n",
    "BEGIN;\n",
    "DELETE FROM PAGE_INFO;\n",
    "COMMIT;\n",
    "DELETE FROM CATEGORY_INFO;\n",
    "COMMIT;\n",
    "DELETE FROM PAGE_DATA;\n",
    "COMMIT;\n",
    "DELETE FROM CATEGORY_DATA;\n",
    "COMMIT;\n",
    "\n",
    "\"\"\"\n",
    "#only execute if you want to clear the table\n",
    "#cursor.execute(clear_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connection, cursor = connect_to_db()\n",
    "\n",
    "\n",
    "def generate_category(category):\n",
    "    '''\n",
    "    format a category for insertion in to a wikipedia api call\n",
    "    '''\n",
    "    category = re.sub('\\s','_',category)\n",
    "    category = category.lower()\n",
    "    return category\n",
    "\n",
    "def generate_query(category):\n",
    "    '''\n",
    "    Format an api call for requests\n",
    "    '''\n",
    "    query = \"\"\"\n",
    "            http://en.wikipedia.org/w/api.php?\n",
    "            action=query&\n",
    "            format=json&\n",
    "            list=categorymembers&\n",
    "            cmtitle=Category:{}& \n",
    "            cmlimit=max\n",
    "            \"\"\".format(generate_category(category))\n",
    "    query = re.sub('\\s','',query)\n",
    "    return query\n",
    "\n",
    "def get_page_ids(category, level=0):\n",
    "    #insert record into the Category Table and get the categoryID\n",
    "    print('Getting PageIDs from Wikipedia for category: {}.  Please wait'.format(category))\n",
    "    category = generate_category(category)\n",
    "    cursor.execute(\"SELECT CATEGORYID FROM CATEGORY_INFO where category_name = '{}';\".format(category))\n",
    "    cat_id = cursor.fetchall()\n",
    "    if not cat_id:\n",
    "        insert_category = \"\"\"\n",
    "                BEGIN;\n",
    "                INSERT INTO CATEGORY_INFO (CATEGORY_NAME) VALUES ('{}');\n",
    "                COMMIT;\n",
    "                    \"\"\".format(category.lower())\n",
    "        cursor.execute(insert_category)\n",
    "    cursor.execute(\"SELECT CATEGORYID FROM CATEGORY_INFO where category_name = '{}';\".format(category))\n",
    "    cat_id = cursor.fetchall()\n",
    "    cat_id = cat_id[0]['categoryid']\n",
    "    get_wikipedia_page_ids(category, cat_id, level)\n",
    "    print('Finished getting PageID list')\n",
    "    get_wikipedia_page_text()\n",
    "    \n",
    "\n",
    "def get_wikipedia_page_ids(category, cat_id, level = 0):\n",
    "    response_url = requests.get(generate_query(category))\n",
    "    response_url.json()\n",
    "    id_list = response_url.json()['query']['categorymembers']\n",
    "    for x in id_list:\n",
    "        pageid = x['pageid']\n",
    "        title = x['title']\n",
    "        title = title.replace(\"'\", '\"')\n",
    "        if ('Category:' not in title):\n",
    "            #check to see if page already exists.  If it does do not insert again\n",
    "            cursor.execute(\"\"\"SELECT PAGEID FROM PAGE_INFO WHERE PAGEID = {};\"\"\".format(pageid))\n",
    "            page_exist = cursor.fetchall()\n",
    "            if not page_exist:\n",
    "                insert_stmt = \"\"\"\n",
    "                       BEGIN;\n",
    "                       INSERT INTO PAGE_INFO VALUES ({},'{}');\n",
    "                       COMMIT;\n",
    "                   \"\"\".format(pageid, title)\n",
    "                cursor.execute(insert_stmt)\n",
    "            #from here, insert category_id and page_id into category_data table\n",
    "            \n",
    "            cursor.execute(\"\"\"SELECT MAINCATEGORYID FROM CATEGORY_DATA WHERE MAINCATEGORYID = {} AND PAGEID = {};\"\"\".format(cat_id, pageid))\n",
    "            rec_exist = cursor.fetchall()\n",
    "            if not rec_exist:\n",
    "                insert_stmt = \"\"\"\n",
    "                        BEGIN;\n",
    "                        INSERT INTO CATEGORY_DATA (MAINCATEGORYID, PAGEID, LEVEL, CATEGORY) VALUES ({}, {}, {}, '{}');\n",
    "                        COMMIT;\n",
    "                    \"\"\".format(cat_id, pageid,level,category)\n",
    "                cursor.execute(insert_stmt)\n",
    "        else:\n",
    "            # if level > 0 decrease level by 1 and then make recursive call using sub-category as category \n",
    "            if level:\n",
    "                sub_cat = title.replace('Category:', '')\n",
    "                cursor.execute(\"\"\"SELECT MAINCATEGORYID, CATEGORY, SUBCATEGORY FROM CATEGORY_DATA\n",
    "                WHERE MAINCATEGORYID = {} AND CATEGORY = '{}' AND SUBCATEGORY = '{}'\"\"\".format(cat_id, category, sub_cat))\n",
    "                rec_exist = cursor.fetchall()\n",
    "                if not rec_exist:\n",
    "                    insert_subcat = \"\"\"\n",
    "                        BEGIN;\n",
    "                        INSERT INTO CATEGORY_DATA (MAINCATEGORYID, CATEGORY, SUBCATEGORY, LEVEL ) VALUES ({}, '{}', '{}',{});\n",
    "                        COMMIT;\n",
    "                        \"\"\".format(cat_id,  category, sub_cat, level)\n",
    "                    cursor.execute(insert_subcat)\n",
    "                    level -=1\n",
    "                    get_wikipedia_page_ids(sub_cat, cat_id, level)\n",
    "                    level +=1     \n",
    "    \n",
    "\n",
    "\n",
    "def get_wikipedia_page_text():\n",
    "    print('Start getting wikipedia page text from PageID list.  Please wait')\n",
    "    select_stmt = \"\"\"\n",
    "            SELECT A.PAGEID,A.TITLE FROM PAGE_INFO A LEFT OUTER JOIN PAGE_DATA B ON A.PAGEID = B.PAGEID WHERE B.PAGEID IS NULL;\n",
    "        \"\"\"\n",
    "    cursor.execute(select_stmt)\n",
    "    result = cursor.fetchall()\n",
    "    for item in result:\n",
    "        pageid = item['pageid']\n",
    "        title = item['title']\n",
    "        rich_url = 'https://en.wikipedia.org/w/api.php?action=query&format=json&prop=extracts&explaintext=True&pageids=' + str(pageid)\n",
    "        response_url = requests.get(rich_url)\n",
    "        r_text = response_url.json()['query']['pages'][str(pageid)]['extract']\n",
    "        r_text = r_text.replace(\"'\", '\"')\n",
    "        r_text = cleaner(r_text)\n",
    "        insert_stmt = \"\"\"\n",
    "                    BEGIN;\n",
    "                    INSERT INTO PAGE_DATA VALUES ({},'{}', '{}');\n",
    "                    COMMIT;\n",
    "        \"\"\".format(pageid, title, r_text)\n",
    "        cursor.execute(insert_stmt)  \n",
    "    print('Finished getting page text from wikipedia.  Process is complete')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_wikipedia_page_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning level 6\n",
    "#Business Software level 2\n",
    "#Maps level 4\n",
    "get_page_ids('Maps', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    get_some_docs = \"\"\"\n",
    "    with tb1 as( SELECT DISTINCT A.PAGEID, A.TITLE, A.PAGE_TEXT, C.CATEGORY_NAME FROM PAGE_DATA A, CATEGORY_DATA B, CATEGORY_INFO C\n",
    "    WHERE A.PAGEID = B.PAGEID AND B.MAINCATEGORYID = C.CATEGORYID AND C.CATEGORY_NAME = 'machine_learning' limit 1100),\n",
    "    tb2 as \n",
    "    (SELECT DISTINCT A.PAGEID, A.TITLE, A.PAGE_TEXT,  C.CATEGORY_NAME FROM PAGE_DATA A, CATEGORY_DATA B, CATEGORY_INFO C\n",
    "    WHERE A.PAGEID = B.PAGEID AND B.MAINCATEGORYID = C.CATEGORYID AND C.CATEGORY_NAME = 'business_software' limit 1100),\n",
    "    tb3 as\n",
    "    (SELECT DISTINCT A.PAGEID, A.TITLE, A.PAGE_TEXT,C.CATEGORY_NAME FROM PAGE_DATA A, CATEGORY_DATA B, CATEGORY_INFO C\n",
    "    WHERE A.PAGEID = B.PAGEID AND B.MAINCATEGORYID = C.CATEGORYID AND C.CATEGORY_NAME = 'maps' limit 1100)\n",
    "    select * from tb1 union all select * from tb2 union all select * from tb3;\n",
    "    \"\"\"\n",
    "    return(query_to_dataframe(get_some_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 500\n",
    "algorithm = 'randomized'\n",
    "random_state = 42\n",
    "ngram_range=(1,2)\n",
    "min_df = 1\n",
    "max_df = .7\n",
    "\n",
    "#make the pipeline\n",
    "svd_pipe = Pipeline([\n",
    "    ('tfidf_vec', TfidfVectorizer( ngram_range, max_df=max_df, min_df=min_df, stop_words = 'english')),\n",
    "    ('trun_svd', TruncatedSVD(n_components=n_components, algorithm = algorithm, random_state = random_state)),\n",
    "    ('normalizer', Normalizer(copy=False))\n",
    "])\n",
    "\n",
    "#fit and transform model\n",
    "#after fit and transform the model is trained and ready to find related documents given search term.\n",
    "df = get_data()\n",
    "svd_matrix = svd_pipe.fit_transform(df['page_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/pkl\n"
     ]
    }
   ],
   "source": [
    "cd '/home/jovyan/pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svd_matrix.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pickle the matrix file so it could be used during prediction\n",
    "joblib.dump(svd_matrix, 'svd_matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svd_pipe.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make the pickle of the model\n",
    "joblib.dump(svd_pipe, 'svd_pipe.pkl') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd_matrix = joblib.load('/home/jovyan/pkl/svd_matrix.pkl')\n",
    "svd_pipe = joblib.load('/home/jovyan/pkl/svd_pipe.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five most relevant wikipedia pages to the search string: dot distribution\n",
      "\n",
      "https://en.wikipedia.org/wiki/Discrete_phase-type_distribution\n",
      "https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling\n",
      "https://en.wikipedia.org/wiki/Dot_distribution_map\n",
      "https://en.wikipedia.org/wiki/Algorithmic_inference\n",
      "https://en.wikipedia.org/wiki/Distribution_learning_theory\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test the code\n",
    "#make the search query into a query vector\n",
    "#search_term = ['Data exploration is an approach machine learning is a science']\n",
    "search_term = ['dot distribution']\n",
    "#print(search_term)\n",
    "df = get_data()\n",
    "query_vector = svd_pipe.transform(search_term)\n",
    "tmp = pd.DataFrame(np.dot(svd_matrix, query_vector.T))\n",
    "df['cosine_distance'] = tmp[0]\n",
    "df.sort_values(['cosine_distance'], ascending=False).head(5)\n",
    "return_df = pd.DataFrame(df.sort_values(['cosine_distance'], ascending=False).head(5))\n",
    "return_df.drop(['category_name', 'page_text', 'cosine_distance'], axis=1, inplace=True)\n",
    "wikipedia_url = 'https://en.wikipedia.org/wiki/'\n",
    "return_df['title'] = [wikipedia_url + re.sub('\\s', '_', i) for i in return_df['title']]\n",
    "print('Five most relevant wikipedia pages to the search string: {}\\n'.format(search_term[0]))\n",
    "for item in return_df['title']:\n",
    "    print(item )\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_some_docs = \"\"\"\n",
    "with tb1 as( SELECT DISTINCT A.PAGEID, A.PAGE_TEXT, C.CATEGORY_NAME FROM PAGE_DATA A, CATEGORY_DATA B, CATEGORY_INFO C\n",
    "WHERE A.PAGEID = B.PAGEID AND B.MAINCATEGORYID = C.CATEGORYID AND C.CATEGORY_NAME = 'machine_learning' limit 1000),\n",
    "tb2 as \n",
    "(SELECT DISTINCT A.PAGEID, A.PAGE_TEXT, C.CATEGORY_NAME FROM PAGE_DATA A, CATEGORY_DATA B, CATEGORY_INFO C\n",
    "WHERE A.PAGEID = B.PAGEID AND B.MAINCATEGORYID = C.CATEGORYID AND C.CATEGORY_NAME = 'business_software' limit 1000),\n",
    "tb3 as\n",
    "(SELECT DISTINCT A.PAGEID, A.PAGE_TEXT, C.CATEGORY_NAME FROM PAGE_DATA A, CATEGORY_DATA B, CATEGORY_INFO C\n",
    "WHERE A.PAGEID = B.PAGEID AND B.MAINCATEGORYID = C.CATEGORYID AND C.CATEGORY_NAME = 'maps' limit 1000)\n",
    "select * from tb1 union all select * from tb2 union all select * from tb3;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = query_to_dataframe(get_some_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_components = 500\n",
    "algorithm = 'randomized'\n",
    "random_state = 42\n",
    "ngram_range=(1,2)\n",
    "min_df = 1\n",
    "max_df = .7\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(ngram_range, max_df = max_df, min_df = min_df, stop_words = 'english')\n",
    "\n",
    "X = tfidf_vec.fit_transform(df['page_text'])\n",
    "\n",
    "#make a new DENSE dataframe called text_df from SPARSE matrix X(from tfidf_vectorizer) and put the column headings\n",
    "#on the dataframe and put the category as index so it can be used easily\n",
    "text_df = pd.DataFrame(X.toarray(), columns=tfidf_vec.get_feature_names())\n",
    "text_df.index = df['category_name']\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "model = mnb.fit(text_df, text_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/pkl\n"
     ]
    }
   ],
   "source": [
    "cd '/home/jovyan/pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prediction_model2.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "joblib.dump(tfidf_vec, 'tfidf_vec2.pkl') \n",
    "joblib.dump(model, 'prediction_model2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vec = joblib.load('/home/jovyan/pkl/tfidf_vec2.pkl')\n",
    "prediction_model = joblib.load('/home/jovyan/pkl/prediction_model2.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_page_query(url):\n",
    "    '''\n",
    "    Format an api call for requests\n",
    "    '''\n",
    "    clean_url = re.sub('\\s','_', url.split('/')[-1])\n",
    "    query = \"\"\"http://en.wikipedia.org/w/api.php?action=query&format=json&prop=extracts&explaintext=True&titles={}\"\"\".format(clean_url)\n",
    "    return query\n",
    "\n",
    "def make_page_prediction(url):\n",
    "    response_url = requests.get(generate_page_query(url))\n",
    "    page_id = list(response_url.json()['query']['pages'].keys())[0]\n",
    "    if int(page_id) == -1:\n",
    "        print ('Invalid URL given or wikipedia page not found')\n",
    "    else:\n",
    "        page_text = response_url.json()['query']['pages'][page_id]['extract']\n",
    "        X = tfidf_vec.transform([cleaner(page_text)])\n",
    "        predicted_category = prediction_model.predict(X)[0]   \n",
    "        proba_dict = {}\n",
    "        for key, val in enumerate(prediction_model.classes_):\n",
    "            proba_dict[val] = prediction_model.predict_proba(X)[0][key]\n",
    "        #print(proba_dict)\n",
    "        \n",
    "    return predicted_category, proba_dict[predicted_category].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: https://en.wikipedia.org/wiki/R_(programming_language) \n",
      "Predicted Category: machine_learning \n",
      "Probability: 0.697\n"
     ]
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Dot_distribution_map'\n",
    "url = 'https://en.wikipedia.org/wiki/R_(programming_language)'\n",
    "cat, proba = make_page_prediction(url)\n",
    "print('Page: {} \\nPredicted Category: {} \\nProbability: {}'.format(url, cat, proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
